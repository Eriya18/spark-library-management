{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Eriya18/spark-library-management/blob/main/Library_ETL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TGi-oEwfLZp-",
        "outputId": "749f01db-297b-4db4-b59c-a8b66c643a88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.12/dist-packages (3.5.1)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.12/dist-packages (18.1.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.12/dist-packages (from pyspark) (0.10.9.7)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Folders ready!\n"
          ]
        }
      ],
      "source": [
        "# Install dependencies\n",
        "!pip install pyspark pyarrow pandas\n",
        "\n",
        "# Create data folders\n",
        "import os\n",
        "\n",
        "BASE = \"/content/data\"\n",
        "STREAM_DIR = f\"{BASE}/stream_input\"\n",
        "\n",
        "os.makedirs(BASE, exist_ok=True)\n",
        "os.makedirs(STREAM_DIR, exist_ok=True)\n",
        "\n",
        "print(\"Folders ready!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python gen_books.py\n",
        "!python gen_users.py\n",
        "!python gen_categories.py\n",
        "!python gen_loans_txt.py\n",
        "!python gen_returns_parquet.py\n",
        "\n",
        "import os\n",
        "print(\"Generated files:\", os.listdir(\"/content/data\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P3hQcm84MUUp",
        "outputId": "c163103b-277e-4bb1-c7cd-58d18e12b12b"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "books.json generated\n",
            "users.csv generated\n",
            "categories.csv generated\n",
            "loans.txt generated\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/spark-library-management/gen_returns_parquet.py\", line 18, in <module>\n",
            "    df.to_parquet('data/returns.parquet', index=False)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pandas/util/_decorators.py\", line 333, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pandas/core/frame.py\", line 3113, in to_parquet\n",
            "    return to_parquet(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pandas/io/parquet.py\", line 480, in to_parquet\n",
            "    impl.write(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pandas/io/parquet.py\", line 228, in write\n",
            "    self.api.parquet.write_table(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pyarrow/parquet/core.py\", line 1902, in write_table\n",
            "    with ParquetWriter(\n",
            "         ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pyarrow/parquet/core.py\", line 1015, in __init__\n",
            "    sink = self.file_handle = filesystem.open_output_stream(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"pyarrow/_fs.pyx\", line 887, in pyarrow._fs.FileSystem.open_output_stream\n",
            "  File \"pyarrow/error.pxi\", line 155, in pyarrow.lib.pyarrow_internal_check_status\n",
            "  File \"pyarrow/error.pxi\", line 92, in pyarrow.lib.check_status\n",
            "IsADirectoryError: [Errno 21] Failed to open local file 'data/returns.parquet'. Detail: [errno 21] Is a directory\n",
            "Generated files: ['stream_input', 'categories.csv', 'stream_output', 'loans.txt', 'users.csv', 'stream_checkpoint', 'returns.parquet', 'books.json']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "shutil.rmtree(\"data/returns.parquet\", ignore_errors=True)\n",
        "print(\"Deleted directory named returns.parquet.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZvQlNiyquzjS",
        "outputId": "e1c1ce52-cb1a-4403-b231-71dd7271fa95"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleted directory named returns.parquet.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Library_ETL_Colab\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "spark.conf.set(\"spark.sql.shuffle.partitions\", \"4\")\n",
        "spark.conf.set(\"spark.sql.session.timeZone\", \"UTC\")\n",
        "\n",
        "print(\"Spark started.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YP7y3C1YMXyT",
        "outputId": "558b9e8e-b5a2-45f9-ddd9-2129a21e28df"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spark started.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Delete old parquet\n",
        "os.remove(\"/content/data/returns.parquet\")\n",
        "print(\"Old returns.parquet deleted.\")\n",
        "\n",
        "# Create Spark-compatible parquet\n",
        "import random\n",
        "from datetime import datetime, timedelta\n",
        "from pyspark.sql.functions import to_timestamp\n",
        "\n",
        "NUM = 120\n",
        "loans=[f\"L{i:06d}\" for i in range(1,301)]\n",
        "users=[f\"U{i:05d}\" for i in range(1,201)]\n",
        "books=[f\"B{i:05d}\" for i in range(1,151)]\n",
        "\n",
        "rows=[]\n",
        "for i in range(NUM):\n",
        "    loan_id=random.choice(loans)\n",
        "    ts = (datetime.now() - timedelta(days=random.randint(0,30))).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "    rows.append((loan_id, random.choice(users), random.choice(books), ts))\n",
        "\n",
        "df = spark.createDataFrame(rows, [\"loan_id\",\"user_id\",\"book_id\",\"returned_at\"])\n",
        "df = df.withColumn(\"returned_at\", to_timestamp(\"returned_at\"))\n",
        "df.write.mode(\"overwrite\").parquet(\"data/returns.parquet\")\n",
        "\n",
        "print(\"New returns.parquet created!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "_2UNeotPMbys",
        "outputId": "f5874615-32ef-49b2-adab-9ff7b690a873"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IsADirectoryError",
          "evalue": "[Errno 21] Is a directory: '/content/data/returns.parquet'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIsADirectoryError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-459490328.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Delete old parquet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/data/returns.parquet\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Old returns.parquet deleted.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIsADirectoryError\u001b[0m: [Errno 21] Is a directory: '/content/data/returns.parquet'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import to_timestamp\n",
        "\n",
        "BASE = \"/content/data\"\n",
        "\n",
        "books = spark.read.option(\"multiline\", \"true\").json(f\"{BASE}/books.json\")\n",
        "users = spark.read.csv(f\"{BASE}/users.csv\", header=True, inferSchema=True)\n",
        "categories = spark.read.csv(f\"{BASE}/categories.csv\", header=True, inferSchema=True)\n",
        "loans = spark.read.option(\"header\", True).option(\"delimiter\", \"|\").csv(f\"{BASE}/loans.txt\")\n",
        "returns = spark.read.parquet(f\"{BASE}/returns.parquet\")\n",
        "\n",
        "print(\"Batch data loaded!\")\n"
      ],
      "metadata": {
        "id": "_aprMu12MiGz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loans = loans.withColumn(\"loan_date\", to_timestamp(\"loan_date\")) \\\n",
        "             .withColumn(\"due_date\", to_timestamp(\"due_date\"))\n",
        "\n",
        "returns = returns.withColumn(\"returned_at\", to_timestamp(\"returned_at\"))\n"
      ],
      "metadata": {
        "id": "pjeDO_SKMpcU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType, StructField, StringType\n",
        "\n",
        "EVENT_SCHEMA = StructType([\n",
        "    StructField(\"event_id\", StringType(), True),\n",
        "    StructField(\"loan_id\", StringType(), True),\n",
        "    StructField(\"user_id\", StringType(), True),\n",
        "    StructField(\"book_id\", StringType(), True),\n",
        "    StructField(\"loan_date\", StringType(), True),\n",
        "    StructField(\"due_date\", StringType(), True),\n",
        "    StructField(\"status\", StringType(), True),\n",
        "])\n"
      ],
      "metadata": {
        "id": "8ugrwPzrMrN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import to_timestamp, expr\n",
        "\n",
        "STREAM_DIR = \"/content/data/stream_input\"\n",
        "OUTPUT = \"/content/data/stream_output\"\n",
        "CHECKPOINT = \"/content/data/stream_checkpoint\"\n",
        "\n",
        "stream_df = (\n",
        "    spark.readStream\n",
        "        .schema(EVENT_SCHEMA)\n",
        "        .json(STREAM_DIR)\n",
        ")\n",
        "\n",
        "stream_df = (\n",
        "    stream_df\n",
        "        .withColumn(\"loan_date\", to_timestamp(\"loan_date\"))\n",
        "        .withColumn(\"due_date\", to_timestamp(\"due_date\"))\n",
        "        .withColumn(\"event_date\", expr(\"date(loan_date)\"))\n",
        ")\n",
        "\n",
        "query = (\n",
        "    stream_df.writeStream\n",
        "        .format(\"parquet\")\n",
        "        .option(\"path\", OUTPUT)\n",
        "        .option(\"checkpointLocation\", CHECKPOINT)\n",
        "        .outputMode(\"append\")\n",
        "        .start()\n",
        ")\n",
        "\n",
        "print(\"ðŸ”¥ Streaming started in background.\")\n"
      ],
      "metadata": {
        "id": "rGrddjMtMtUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json, uuid, time\n",
        "\n",
        "for i in range(3):\n",
        "    event = {\n",
        "        \"event_id\": str(uuid.uuid4()),\n",
        "        \"loan_id\": f\"LS{i:05d}\",\n",
        "        \"user_id\": \"U00001\",\n",
        "        \"book_id\": \"B00001\",\n",
        "        \"loan_date\": \"2025-01-01 10:00:00\",\n",
        "        \"due_date\": \"2025-01-14 10:00:00\",\n",
        "        \"status\": \"borrowed\"\n",
        "    }\n",
        "\n",
        "    with open(f\"/content/data/stream_input/event_{i}.json\", \"w\") as f:\n",
        "        json.dump(event, f)\n",
        "\n",
        "    print(\"Generated:\", event)\n",
        "    time.sleep(2)\n"
      ],
      "metadata": {
        "id": "R9AymWBgMyFg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.parquet(\"/content/data/stream_output\")\n",
        "df.show(10, truncate=False)\n"
      ],
      "metadata": {
        "id": "ncPF5EG3M4tr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "loans_books = loans.join(books, on=\"book_id\", how=\"left\")\n",
        "loans_books_users = loans_books.join(users, on=\"user_id\", how=\"left\")\n",
        "loans_full = loans_books_users.join(categories, on=\"category_id\", how=\"left\")\n",
        "loans_with_returns = loans_full.join(\n",
        "    returns,\n",
        "    on=[\"loan_id\", \"user_id\", \"book_id\"],\n",
        "    how=\"left\"\n",
        ")\n",
        "\n",
        "stream_events = spark.read.parquet(\"/content/data/stream_output\")\n",
        "\n",
        "all_loans_final = loans_with_returns.unionByName(\n",
        "    stream_events, allowMissingColumns=True\n",
        ")\n",
        "\n",
        "all_loans_final.show(20, truncate=False)\n"
      ],
      "metadata": {
        "id": "ZQ8vnK2hM7g_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dash==2.11.1 jupyter-dash pyngrok plotly pandas pyarrow fastparquet\n"
      ],
      "metadata": {
        "id": "zbiSD21MO_Sk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "from pyngrok.exception import PyngrokNgrokHTTPError\n",
        "import time\n",
        "\n",
        "ngrok.set_auth_token(\"35lkbwn07pXhttFG2rebGL1aO5x_7Sek5qRH7QNSwqoXkYxq\")\n",
        "\n",
        "public_url = None\n",
        "max_retries = 5 # Increased max retries\n",
        "retry_delay_seconds = 15 # Increased delay between connection attempts\n",
        "\n",
        "for attempt in range(max_retries):\n",
        "    print(f\"Attempt {attempt + 1} to connect ngrok...\")\n",
        "    try:\n",
        "        ngrok.kill()\n",
        "        # Wait for the ngrok process to fully terminate and for the server-side endpoint to potentially clear.\n",
        "        time.sleep(retry_delay_seconds) # Use the same delay for cleanup before attempting connection\n",
        "        public_url = ngrok.connect(8050)\n",
        "        print(\"ðŸ”— Dashboard URL:\", public_url)\n",
        "        break # Success, exit loop\n",
        "    except PyngrokNgrokHTTPError as e:\n",
        "        if \"already online\" in str(e) or \"ERR_NGROK_334\" in str(e):\n",
        "            print(f\"Ngrok endpoint still online. Retrying in {retry_delay_seconds} seconds...\")\n",
        "            if attempt < max_retries - 1: # Only sleep if more retries are pending\n",
        "                time.sleep(retry_delay_seconds)\n",
        "            else: # If max retries reached for this specific error\n",
        "                print(\"Max retries reached for 'endpoint already online' error.\")\n",
        "                break\n",
        "        else:\n",
        "            print(f\"An unexpected ngrok error occurred: {e}\")\n",
        "            break # Other error, no point in retrying\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "        break\n",
        "\n",
        "if public_url is None:\n",
        "    print(\"Failed to establish ngrok tunnel after multiple attempts.\")"
      ],
      "metadata": {
        "id": "VG6eB0GmRRnH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import dash\n",
        "from dash import html, dcc, Output, Input\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import os\n",
        "\n",
        "# Data folder\n",
        "DATA_DIR = \"/content/data/stream_output\"\n",
        "\n",
        "app = dash.Dash(__name__)\n",
        "server = app.server\n",
        "\n",
        "def load_data():\n",
        "    if not os.path.exists(DATA_DIR):\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    files = [os.path.join(DATA_DIR, f) for f in os.listdir(DATA_DIR) if f.endswith(\".parquet\")]\n",
        "    if not files:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    df = pd.concat([pd.read_parquet(f) for f in files], ignore_index=True)\n",
        "    return df\n",
        "\n",
        "app.layout = html.Div([\n",
        "    html.H2(\"ðŸ“˜ Real-Time Library Dashboard\"),\n",
        "    dcc.Interval(id=\"refresh\", interval=5000, n_intervals=0),\n",
        "    dcc.Graph(id=\"live_chart\")\n",
        "])\n",
        "\n",
        "@app.callback(\n",
        "    Output(\"live_chart\", \"figure\"),\n",
        "    Input(\"refresh\", \"n_intervals\")\n",
        ")\n",
        "def update_graph(n):\n",
        "    df = load_data()\n",
        "    if df.empty:\n",
        "        return px.bar(title=\"No streaming data yet\")\n",
        "\n",
        "    return px.bar(df, x=\"book_id\", title=\"Live Streaming Events Count\")\n",
        "\n"
      ],
      "metadata": {
        "id": "S6co_HElRcft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "app.run_server(host=\"0.0.0.0\", port=8050)\n"
      ],
      "metadata": {
        "id": "hRfP0kuERg0D"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}